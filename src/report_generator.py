"""æŠ¥å‘Šç”Ÿæˆæ¨¡å—ï¼šç”Ÿæˆæ—¥æŠ¥å’Œ AI æ‘˜è¦"""
import logging
import sqlite3
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ai_client import AISummaryError, call_chat_analysis
from config import Config
from constants import (
    CATEGORY_PRIORITY,
    DEFAULT_CATEGORY_PRIORITY,
    MIN_THREAD_MESSAGES,
    TOP_N_MESSAGE_IDS,
    TOP_N_THREADS,
    TOP_N_USERS,
    TOP_THREAD_ID,
)
from database import get_replied_message
from message_handler import format_user

log = logging.getLogger(__name__)


def _calculate_statistics(rows: List[sqlite3.Row]) -> Tuple[Dict[str, int], Dict[str, int], Dict[int, int]]:
    """
    è®¡ç®—æ¶ˆæ¯ç»Ÿè®¡æ•°æ®
    
    Args:
        rows: æ¶ˆæ¯è¡Œåˆ—è¡¨
    
    Returns:
        (user_stats, media_stats, thread_stats) å…ƒç»„
    """
    user_stats: Dict[str, int] = {}
    media_stats: Dict[str, int] = {}
    thread_stats: Dict[int, int] = {}

    for row in rows:
        key = format_user(row["user_id"], row["username"])
        user_stats[key] = user_stats.get(key, 0) + 1
        if row["media_type"]:
            media_stats[row["media_type"]] = media_stats.get(row["media_type"], 0) + 1
        if row["reply_to"]:
            thread_stats[row["reply_to"]] = thread_stats.get(row["reply_to"], 0) + 1

    return user_stats, media_stats, thread_stats


def _build_report_header(
    day_start: datetime, day_end: datetime, chat_id: int, chat_name: Optional[str], total: int, user_count: int
) -> List[str]:
    """
    æ„å»ºæŠ¥å‘Šå¤´éƒ¨
    
    Args:
        day_start: æŠ¥å‘Šå¼€å§‹æ—¶é—´
        day_end: æŠ¥å‘Šç»“æŸæ—¶é—´
        chat_id: ç¾¤ç»„ID
        chat_name: ç¾¤ç»„åç§°
        total: æ€»æ¶ˆæ¯æ•°
        user_count: å‘è¨€äººæ•°
    
    Returns:
        æŠ¥å‘Šå¤´éƒ¨è¡Œåˆ—è¡¨
    """
    lines = []
    date_str = day_start.date().isoformat()
    chat_display_name = chat_name or f"ç¾¤ç»„ {chat_id}"
    
    # æ ¼å¼åŒ–æ—¥æœŸæ˜¾ç¤º
    weekday_map = {0: "å‘¨ä¸€", 1: "å‘¨äºŒ", 2: "å‘¨ä¸‰", 3: "å‘¨å››", 4: "å‘¨äº”", 5: "å‘¨å…­", 6: "å‘¨æ—¥"}
    weekday = weekday_map[day_start.weekday()]
    date_display = f"{date_str} {weekday}"
    
    # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´ï¼ˆåªæ˜¾ç¤ºæ—¥æœŸå’Œæ—¶é—´ï¼Œä¸æ˜¾ç¤ºæ—¶åŒºï¼‰
    time_start = day_start.strftime("%H:%M")
    time_end = day_end.strftime("%H:%M")
    
    lines.append(f"# ğŸ“Š {date_display} {chat_display_name} æ—¥æŠ¥")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("### ğŸ“‹ åŸºæœ¬ä¿¡æ¯")
    lines.append("")
    lines.append("| é¡¹ç›® | å†…å®¹ |")
    lines.append("|------|------|")
    lines.append(f"| **ç¾¤ç»„åç§°** | {chat_display_name} |")
    lines.append(f"| **ç¾¤ç»„ ID** | `{chat_id}` |")
    lines.append(f"| **æŠ¥å‘Šæ—¥æœŸ** | {date_str} ({weekday}) |")
    lines.append(f"| **æ—¶é—´èŒƒå›´** | {time_start} ~ {time_end} |")
    lines.append(f"| **æ€»æ¶ˆæ¯æ•°** | **{total}** æ¡ |")
    lines.append(f"| **å‘è¨€äººæ•°** | **{user_count}** äºº |")
    lines.append("")
    lines.append("---")
    lines.append("")
    return lines


def _build_report_content(
    user_stats: Dict[str, int], 
    media_stats: Dict[str, int], 
    thread_stats: Dict[int, int],
    conn: sqlite3.Connection,
    chat_id: int,
    chat_link: Optional[str] = None,
) -> List[str]:
    """
    æ„å»ºæŠ¥å‘Šå†…å®¹éƒ¨åˆ†ï¼ˆæ´»è·ƒç”¨æˆ·ã€åª’ä½“åˆ†å¸ƒï¼‰
    
    Args:
        user_stats: ç”¨æˆ·ç»Ÿè®¡
        media_stats: åª’ä½“ç»Ÿè®¡
        thread_stats: çº¿ç¨‹ç»Ÿè®¡ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹æ¥å£ï¼‰
        conn: æ•°æ®åº“è¿æ¥
        chat_id: ç¾¤ç»„ID
        chat_link: ç¾¤ç»„é“¾æ¥ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æŠ¥å‘Šå†…å®¹è¡Œåˆ—è¡¨
    """
    lines = []
    top_users = sorted(user_stats.items(), key=lambda x: x[1], reverse=True)[:TOP_N_USERS]

    # æ´»è·ƒç”¨æˆ· Top 5
    lines.append("## ğŸ‘¥ æ´»è·ƒç”¨æˆ· Top 5")
    lines.append("")
    if top_users:
        lines.append("| æ’å | ç”¨æˆ·å | æ¶ˆæ¯æ•° |")
        lines.append("|------|--------|--------|")
        rank_icons = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"]
        for idx, (name, cnt) in enumerate(top_users):
            rank_icon = rank_icons[idx] if idx < len(rank_icons) else f"{idx + 1}."
            lines.append(f"| {rank_icon} | {name} | **{cnt}** |")
    else:
        lines.append("*æš‚æ— æ´»è·ƒç”¨æˆ·æ•°æ®*")
    lines.append("")

    # åª’ä½“åˆ†å¸ƒ
    lines.append("## ğŸ“ åª’ä½“åˆ†å¸ƒ")
    lines.append("")
    if media_stats:
        # åª’ä½“ç±»å‹æ˜¾ç¤ºåç§°æ˜ å°„
        media_display_names = {
            "MessageMediaPhoto": "ğŸ“· å›¾ç‰‡",
            "MessageMediaDocument": "ğŸ“„ æ–‡æ¡£",
            "MessageMediaWebPage": "ğŸ”— ç½‘é¡µé“¾æ¥",
            "MessageMediaPoll": "ğŸ“Š æŠ•ç¥¨",
            "MessageMediaVideo": "ğŸ¥ è§†é¢‘",
            "MessageMediaAudio": "ğŸµ éŸ³é¢‘",
            "MessageMediaVoice": "ğŸ¤ è¯­éŸ³",
        }
        
        lines.append("| åª’ä½“ç±»å‹ | æ•°é‡ |")
        lines.append("|----------|------|")
        total_media = sum(media_stats.values())
        for media_type, cnt in sorted(media_stats.items(), key=lambda x: x[1], reverse=True):
            display_name = media_display_names.get(media_type, f"ğŸ“ {media_type}")
            percentage = (cnt / total_media * 100) if total_media > 0 else 0
            lines.append(f"| {display_name} | **{cnt}** ({percentage:.1f}%) |")
    else:
        lines.append("*ä»Šæ—¥æ— åª’ä½“æ¶ˆæ¯*")
    lines.append("")

    return lines


def generate_report(
    conn: sqlite3.Connection,
    cfg: Config,
    day_start: datetime,
    chat_id: int,
    chat_name: Optional[str] = None,
    chat_type: Optional[str] = None,
    chat_link: Optional[str] = None,
    min_thread_messages: Optional[int] = None,
) -> str:
    """ä¸ºæŒ‡å®šç¾¤ç»„ç”Ÿæˆæ—¥æŠ¥"""
    day_end = day_start + timedelta(days=1)
    tz_name = getattr(cfg.timezone, "key", None) or str(cfg.timezone)
    day_start_utc = day_start.astimezone(timezone.utc)
    day_end_utc = day_end.astimezone(timezone.utc)

    # è®°å½•æŸ¥è¯¢æ—¶é—´èŒƒå›´ç”¨äºæ’æŸ¥
    chat_display_name = chat_name or f"chat_{chat_id}"
    log.info("Generating report for %s (chat_id: %s)", chat_display_name, chat_id)
    log.info(
        "Report time range (%s): %s ~ %s",
        tz_name,
        day_start.isoformat(),
        day_end.isoformat(),
    )
    log.info(
        "Report time range (UTC): %s ~ %s",
        day_start_utc.isoformat(),
        day_end_utc.isoformat(),
    )

    conn.row_factory = sqlite3.Row
    cur = conn.execute(
        """
        SELECT message_id, user_id, username, text, media_type, reply_to, date, thread_id
        FROM messages
        WHERE chat_id = ? AND date >= ? AND date < ?
        ORDER BY date ASC
        """,
        (chat_id, day_start.isoformat(), day_end.isoformat()),
    )
    rows = cur.fetchall()
    total = len(rows)

    log.info("Found %s messages in database for time range", total)

    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    user_stats, media_stats, thread_stats = _calculate_statistics(rows)

    # æ„å»ºæŠ¥å‘Š
    lines = _build_report_header(day_start, day_end, chat_id, chat_name, total, len(user_stats))
    lines.extend(_build_report_content(user_stats, media_stats, thread_stats, conn, chat_id, chat_link))

    lines.extend(build_ai_summary_section(rows, cfg, day_start, chat_id, chat_name, chat_type, chat_link, conn, min_thread_messages))

    report = "\n".join(lines)
    # æŠ¥å‘Šæ–‡ä»¶ååŒ…å« chat_idï¼Œå¦‚æœæœ‰åç§°åˆ™ä½¿ç”¨åç§°ï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
    date_str = day_start.date().isoformat()
    if chat_name:
        safe_name = "".join(c if c.isalnum() or c in ("-", "_") else "_" for c in chat_name)
        report_filename = f"{date_str}_{safe_name}_{chat_id}.md"
    else:
        report_filename = f"{date_str}_{chat_id}.md"
    report_path = cfg.report_dir / report_filename
    report_path.write_text(report, encoding="utf-8")
    log.info("Report written to %s", report_path)
    return report


def _group_messages_by_thread(rows: List[sqlite3.Row]) -> Dict[int, List[sqlite3.Row]]:
    """
    æŒ‰ thread_id åˆ†ç»„æ¶ˆæ¯ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªçº¿ç¨‹å†…çš„æ¶ˆæ¯æŒ‰æ—¶é—´é¡ºåºæ’åº
    
    Args:
        rows: æ¶ˆæ¯è¡Œåˆ—è¡¨ï¼ˆåº”è¯¥å·²ç»æŒ‰æ—¶é—´æ’åºï¼‰
    
    Returns:
        æŒ‰ thread_id åˆ†ç»„çš„æ¶ˆæ¯å­—å…¸ï¼Œæ¯ä¸ªçº¿ç¨‹å†…çš„æ¶ˆæ¯æŒ‰æ—¶é—´é¡ºåºæ’åº
    """
    threads: Dict[int, List[sqlite3.Row]] = {}
    for row in rows:
        thread_id = row["thread_id"]
        if thread_id not in threads:
            threads[thread_id] = []
        threads[thread_id].append(row)
    
    # ç¡®ä¿æ¯ä¸ªçº¿ç¨‹å†…çš„æ¶ˆæ¯æŒ‰æ—¶é—´é¡ºåºæ’åº
    for thread_id in threads:
        threads[thread_id].sort(key=lambda r: r["date"])
    
    return threads


def _convert_rows_to_messages(
    rows: List[sqlite3.Row], 
    conn: sqlite3.Connection, 
    chat_id: int
) -> List[Dict[str, Any]]:
    """
    å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºæ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´çš„å›å¤å…³ç³»ä¿¡æ¯
    
    å¦‚æœæ¶ˆæ¯å›å¤äº†ä¸åœ¨æ•°æ®åº“ä¸­çš„æ¶ˆæ¯ï¼Œåˆ™è·³è¿‡è¯¥æ¶ˆæ¯ï¼ˆä¸å‘é€ç»™AIï¼‰
    ç¡®ä¿è¿”å›çš„æ¶ˆæ¯åˆ—è¡¨æŒ‰æ—¶é—´é¡ºåºæ’åºã€‚
    
    Args:
        rows: æ¶ˆæ¯è¡Œåˆ—è¡¨ï¼ˆåº”è¯¥å·²ç»æŒ‰æ—¶é—´æ’åºï¼‰
        conn: æ•°æ®åº“è¿æ¥
        chat_id: ç¾¤ç»„ID
    
    Returns:
        æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«å›å¤å…³ç³»ä¿¡æ¯ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åº
    """
    # ç¡®ä¿è¾“å…¥çš„æ¶ˆæ¯æŒ‰æ—¶é—´æ’åº
    sorted_rows = sorted(rows, key=lambda r: r["date"])
    
    messages = []
    skipped_count = 0
    for row in sorted_rows:
        msg_dict = {
            "id": row["message_id"],
            "user": format_user(row["user_id"], row["username"]),
            "ts": row["date"],
            "text": row["text"] or "",
            "media_type": row["media_type"],
            "reply_to": row["reply_to"],
        }
        
        # å¦‚æœæ¶ˆæ¯æœ‰å›å¤å…³ç³»ï¼ŒæŸ¥è¯¢è¢«å›å¤çš„æ¶ˆæ¯è¯¦æƒ…
        if row["reply_to"]:
            replied_msg = get_replied_message(conn, chat_id, row["reply_to"])
            if replied_msg:
                # è¢«å›å¤çš„æ¶ˆæ¯åœ¨æ•°æ®åº“ä¸­ï¼ŒåŒ…å«å®Œæ•´ä¿¡æ¯
                msg_dict["replied_message"] = {
                    "id": replied_msg["message_id"],
                    "user": format_user(replied_msg["user_id"], replied_msg["username"]),
                    "text": replied_msg["text"] or "",
                    "media_type": replied_msg["media_type"],
                    "ts": replied_msg["date"],
                }
                messages.append(msg_dict)
            else:
                # è¢«å›å¤çš„æ¶ˆæ¯ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œè·³è¿‡è¿™æ¡å›å¤æ¶ˆæ¯
                skipped_count += 1
                log.debug("Skipping message %s: replied message %s not in database", row["message_id"], row["reply_to"])
                continue
        else:
            # æ²¡æœ‰å›å¤å…³ç³»ï¼Œç›´æ¥æ·»åŠ 
            messages.append(msg_dict)
    
    if skipped_count > 0:
        log.info("Skipped %s messages with replied messages not in database", skipped_count)
    
    return messages


def _build_ai_payload(
    chat_id: int,
    chat_name: Optional[str],
    chat_type: Optional[str],
    day_start: datetime,
    tz_name: str,
    thread_id: int,
    messages: List[Dict[str, Any]],
    cfg: Config,
    batch_info: Optional[str] = None,
) -> Dict[str, Any]:
    """
    æ„å»º AI åˆ†æè¯·æ±‚çš„ payload
    
    Args:
        chat_id: ç¾¤ç»„ID
        chat_name: ç¾¤ç»„åç§°
        chat_type: ç¾¤ç»„ç±»å‹
        day_start: æŠ¥å‘Šå¼€å§‹æ—¶é—´
        tz_name: æ—¶åŒºåç§°
        thread_id: çº¿ç¨‹ID
        messages: æ¶ˆæ¯åˆ—è¡¨
        cfg: é…ç½®å¯¹è±¡
        batch_info: æ‰¹æ¬¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        AI è¯·æ±‚ payload
    """
    payload: Dict[str, Any] = {
        "chat_id": chat_id,
        "chat_name": chat_name,
        "chat_type": chat_type,
        "date": day_start.date().isoformat(),
        "timezone": tz_name,
        "thread_id": thread_id,
        "messages": messages,
    }
    if batch_info:
        payload["batch_info"] = batch_info
    if cfg.ai_max_categories:
        payload["max_categories"] = cfg.ai_max_categories
    if cfg.ai_style:
        payload["style"] = cfg.ai_style
    return payload


def _sort_categories_by_priority(categories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æŒ‰ä¼˜å…ˆçº§å¯¹åˆ†ç±»è¿›è¡Œæ’åº
    
    Args:
        categories: åˆ†ç±»åˆ—è¡¨
    
    Returns:
        æ’åºåçš„åˆ†ç±»åˆ—è¡¨
    """
    def get_priority(cat: Dict[str, Any]) -> int:
        cat_name = cat.get("name", "")
        return CATEGORY_PRIORITY.get(cat_name, DEFAULT_CATEGORY_PRIORITY)

    return sorted(categories, key=get_priority)


def _merge_categories(categories_list: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    åˆå¹¶ç›¸åŒåç§°çš„åˆ†ç±»
    
    Args:
        categories_list: åˆ†ç±»åˆ—è¡¨
    
    Returns:
        åˆå¹¶åçš„åˆ†ç±»å­—å…¸ï¼Œkey ä¸ºåˆ†ç±»åç§°ï¼Œvalue åŒ…å«åˆå¹¶åçš„æ¶ˆæ¯IDå’Œæ‘˜è¦åˆ—è¡¨
    """
    category_map: Dict[str, Dict[str, Any]] = {}
    for cat in categories_list:
        name = cat.get("name") or "æœªå‘½ååˆ†ç±»"
        if name not in category_map:
            category_map[name] = {"message_ids": [], "summaries": []}
        
        summary = cat.get("summary") or ""
        if summary:
            category_map[name]["summaries"].append(summary)
        
        msg_ids = cat.get("messages") or []
        category_map[name]["message_ids"].extend(msg_ids)
    
    # å»é‡æ¶ˆæ¯ID
    for name in category_map:
        category_map[name]["message_ids"] = list(dict.fromkeys(category_map[name]["message_ids"]))
    
    return category_map


def _format_category_output(
    category_map: Dict[str, Dict[str, Any]], 
    is_batch: bool = False,
    chat_link: Optional[str] = None,
    message_map: Optional[Dict[int, Dict[str, Any]]] = None,
) -> List[str]:
    """
    æ ¼å¼åŒ–åˆ†ç±»è¾“å‡º
    
    Args:
        category_map: åˆå¹¶åçš„åˆ†ç±»å­—å…¸
        is_batch: æ˜¯å¦ä¸ºæ‰¹æ¬¡å¤„ç†
        chat_link: ç¾¤ç»„é“¾æ¥ï¼Œç”¨äºç”Ÿæˆæ¶ˆæ¯é“¾æ¥
        message_map: æ¶ˆæ¯IDåˆ°æ¶ˆæ¯è¯¦æƒ…çš„æ˜ å°„
    
    Returns:
        æ ¼å¼åŒ–çš„åˆ†ç±»è¾“å‡ºè¡Œåˆ—è¡¨
    """
    lines = []
    
    def get_priority(cat_name: str) -> int:
        return CATEGORY_PRIORITY.get(cat_name, DEFAULT_CATEGORY_PRIORITY)
    
    sorted_names = sorted(category_map.keys(), key=get_priority)
    
    lines.append("#### ğŸ“‚ åˆ†ç±»è¯¦æƒ…")
    if is_batch:
        lines.append("*ï¼ˆåˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ï¼‰*")
    lines.append("")
    
    # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯IDç”¨äºåŸå§‹å¼•ç”¨éƒ¨åˆ†
    all_message_refs: List[Tuple[int, str]] = []
    
    for name in sorted_names:
        cat_data = category_map[name]
        message_ids = cat_data["message_ids"]
        summaries = cat_data["summaries"]
        
        # åˆ†ç±»æ ‡é¢˜
        lines.append(f"##### ğŸ”¸ {name}")
        lines.append("")
        
        if summaries:
            if is_batch and len(summaries) > 1:
                # åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„æ‘˜è¦
                combined_summary = " | ".join(summaries)
            else:
                combined_summary = summaries[0]
            
            summary_lines = combined_summary.split("\n")
            for line in summary_lines:
                if line.strip():
                    lines.append(f"{line}")
            lines.append("")
        
        # æ”¶é›†è¯¥åˆ†ç±»çš„æ‰€æœ‰æ¶ˆæ¯å¼•ç”¨
        for msg_id in message_ids:
            if message_map and msg_id in message_map:
                msg_info = message_map[msg_id]
                text = msg_info.get("text", "").strip()
                # æˆªå–æ¶ˆæ¯æ–‡æœ¬çš„å‰50ä¸ªå­—ç¬¦ï¼Œå¦‚æœå¤ªé•¿åˆ™æ·»åŠ çœç•¥å·
                if text:
                    display_text = text[:50] + ("..." if len(text) > 50 else "")
                    # è½¬ä¹‰Markdowné“¾æ¥æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…ç ´åé“¾æ¥è¯­æ³•
                    # åœ¨é“¾æ¥æ–‡æœ¬ä¸­ï¼Œ] å’Œ ) éœ€è¦è½¬ä¹‰ï¼Œå¦åˆ™ä¼šç ´åé“¾æ¥è¯­æ³•
                    display_text = display_text.replace("]", "\\]").replace(")", "\\)")
                else:
                    display_text = "[åª’ä½“æ¶ˆæ¯]"
                
                all_message_refs.append((msg_id, display_text))
            else:
                all_message_refs.append((msg_id, ""))
    
    # æ·»åŠ åŸå§‹å¼•ç”¨åˆ†ç±»
    if all_message_refs:
        lines.append("---")
        lines.append("")
        lines.append("#### ğŸ“ åŸå§‹æ¶ˆæ¯å¼•ç”¨")
        lines.append("")
        # å»é‡æ¶ˆæ¯IDï¼ˆä¿æŒé¡ºåºï¼‰
        seen_ids = set()
        unique_refs = []
        for msg_id, display_text in all_message_refs:
            if msg_id not in seen_ids:
                seen_ids.add(msg_id)
                unique_refs.append((msg_id, display_text))
        
        # ä¸ºæ¯ä¸ªæ¶ˆæ¯ç”Ÿæˆé“¾æ¥
        for idx, (msg_id, display_text) in enumerate(unique_refs, 1):
            if chat_link:
                msg_link = f"{chat_link}/{msg_id}"
                if display_text:
                    # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ä»¥é¿å…ç ´åMarkdownæ ¼å¼ï¼ˆè¡¨æ ¼ä¸­çš„ | éœ€è¦è½¬ä¹‰ï¼‰
                    safe_text = display_text.replace("|", "\\|")
                    lines.append(f"{idx}. [{msg_id}]({msg_link})ï¼š{safe_text}")
                else:
                    lines.append(f"{idx}. [{msg_id}]({msg_link})")
            else:
                if display_text:
                    safe_text = display_text.replace("|", "\\|")
                    lines.append(f"{idx}. {msg_id}ï¼š{safe_text}")
                else:
                    lines.append(f"{idx}. {msg_id}")
        lines.append("")
    
    return lines


def _process_single_thread(
    thread_rows: List[sqlite3.Row],
    thread_id: int,
    cfg: Config,
    day_start: datetime,
    tz_name: str,
    chat_id: int,
    chat_name: Optional[str],
    chat_type: Optional[str],
    chat_link: Optional[str],
    message_map: Dict[int, Dict[str, Any]],
    conn: sqlite3.Connection,
) -> List[str]:
    """
    å¤„ç†å•ä¸ªçº¿ç¨‹ï¼ˆä¸åˆ†æ‰¹ï¼‰
    
    Args:
        thread_rows: çº¿ç¨‹æ¶ˆæ¯è¡Œ
        thread_id: çº¿ç¨‹ID
        cfg: é…ç½®å¯¹è±¡
        day_start: æŠ¥å‘Šå¼€å§‹æ—¶é—´
        tz_name: æ—¶åŒºåç§°
        chat_id: ç¾¤ç»„ID
        chat_name: ç¾¤ç»„åç§°
        chat_type: ç¾¤ç»„ç±»å‹
        conn: æ•°æ®åº“è¿æ¥
    
    Returns:
        å¤„ç†ç»“æœè¡Œåˆ—è¡¨
    """
    lines = []
    messages = _convert_rows_to_messages(thread_rows, conn, chat_id)
    payload = _build_ai_payload(
        chat_id, chat_name, chat_type, day_start, tz_name, thread_id, messages, cfg
    )

    log.info(
        "Calling AI summary for thread %s: base=%s model=%s messages=%s",
        thread_id,
        cfg.ai_api_base,
        cfg.ai_model,
        len(messages),
    )
    try:
        data = call_chat_analysis(
            cfg.ai_api_base, cfg.ai_api_key, payload, model=cfg.ai_model, timeout=cfg.ai_timeout
        )
    except AISummaryError as exc:
        lines.append("âš ï¸ **AI æ‘˜è¦ç”Ÿæˆå¤±è´¥**")
        lines.append("")
        lines.append(f"é”™è¯¯ä¿¡æ¯ï¼š{exc}")
        lines.append("")
        log.warning("AI summary failed for thread %s: %s", thread_id, exc)
        return lines

    overall = data.get("overall")
    if overall:
        lines.append("#### ğŸ“ æ€»è§ˆ")
        lines.append("")
        lines.append(f"> {overall}")
        lines.append("")

    categories = data.get("categories") or []
    if categories:
        sorted_categories = _sort_categories_by_priority(categories)
        category_map = _merge_categories(sorted_categories)
        lines.extend(_format_category_output(category_map, is_batch=False, chat_link=chat_link, message_map=message_map))
    else:
        lines.append("*æœªè¿”å›åˆ†ç±»ç»“æœ*")
        lines.append("")

    return lines


def _process_thread_batch(
    thread_rows: List[sqlite3.Row],
    thread_id: int,
    cfg: Config,
    day_start: datetime,
    tz_name: str,
    chat_id: int,
    chat_name: Optional[str],
    chat_type: Optional[str],
    chat_link: Optional[str],
    message_map: Dict[int, Dict[str, Any]],
    conn: sqlite3.Connection,
) -> List[str]:
    """
    å¤„ç†çº¿ç¨‹çš„æ‰¹æ¬¡ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    
    Args:
        thread_rows: çº¿ç¨‹æ¶ˆæ¯è¡Œ
        thread_id: çº¿ç¨‹ID
        cfg: é…ç½®å¯¹è±¡
        day_start: æŠ¥å‘Šå¼€å§‹æ—¶é—´
        tz_name: æ—¶åŒºåç§°
        chat_id: ç¾¤ç»„ID
        chat_name: ç¾¤ç»„åç§°
        chat_type: ç¾¤ç»„ç±»å‹
        conn: æ•°æ®åº“è¿æ¥
    
    Returns:
        å¤„ç†ç»“æœè¡Œåˆ—è¡¨
    """
    lines = []
    total_messages = len(thread_rows)
    num_batches = (total_messages + cfg.ai_max_messages_per_batch - 1) // cfg.ai_max_messages_per_batch
    
    lines.append("#### âš™ï¸ æ‰¹æ¬¡å¤„ç†ä¿¡æ¯")
    lines.append("")
    lines.append(f"æ¶ˆæ¯æ•°é‡è¾ƒå¤šï¼Œå°†åˆ†æˆ **{num_batches}** ä¸ªæ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š {cfg.ai_max_messages_per_batch} æ¡ï¼‰")
    lines.append("")

    all_overalls: List[str] = []
    all_categories: List[Dict[str, Any]] = []
    batch_failed = False

    # åˆ†æ®µå¤„ç†
    for batch_idx in range(num_batches):
        start_idx = batch_idx * cfg.ai_max_messages_per_batch
        end_idx = min(start_idx + cfg.ai_max_messages_per_batch, total_messages)
        batch_rows = thread_rows[start_idx:end_idx]
        batch_num = batch_idx + 1

        messages = _convert_rows_to_messages(batch_rows, conn, chat_id)
        payload = _build_ai_payload(
            chat_id,
            chat_name,
            chat_type,
            day_start,
            tz_name,
            thread_id,
            messages,
            cfg,
            batch_info=f"æ‰¹æ¬¡ {batch_num}/{num_batches}ï¼Œå…± {total_messages} æ¡æ¶ˆæ¯",
        )

        log.info(
            "Calling AI summary for thread %s batch %s/%s: base=%s model=%s messages=%s",
            thread_id,
            batch_num,
            num_batches,
            cfg.ai_api_base,
            cfg.ai_model,
            len(messages),
        )
        try:
            data = call_chat_analysis(
                cfg.ai_api_base, cfg.ai_api_key, payload, model=cfg.ai_model, timeout=cfg.ai_timeout
            )
        except AISummaryError as exc:
            lines.append(f"âš ï¸ **æ‰¹æ¬¡ {batch_num} AI æ‘˜è¦ç”Ÿæˆå¤±è´¥**ï¼š{exc}")
            lines.append("")
            log.warning("AI summary failed for thread %s batch %s: %s", thread_id, batch_num, exc)
            batch_failed = True
            continue

        batch_overall = data.get("overall")
        if batch_overall:
            all_overalls.append(f"æ‰¹æ¬¡ {batch_num}: {batch_overall}")

        batch_categories = data.get("categories") or []
        if batch_categories:
            all_categories.extend(batch_categories)

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    if batch_failed and not all_overalls and not all_categories:
        lines.append("âš ï¸ **æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å¤±è´¥**")
        lines.append("")
    else:
        if all_overalls:
            lines.append("#### ğŸ“ æ€»è§ˆï¼ˆå„æ‰¹æ¬¡æ‘˜è¦ï¼‰")
            lines.append("")
            for overall in all_overalls:
                lines.append(f"- {overall}")
            lines.append("")

        if all_categories:
            category_map = _merge_categories(all_categories)
            lines.extend(_format_category_output(category_map, is_batch=True, chat_link=chat_link, message_map=message_map))

    return lines


def build_ai_summary_section(
    rows: List[sqlite3.Row],
    cfg: Config,
    day_start: datetime,
    chat_id: int,
    chat_name: Optional[str] = None,
    chat_type: Optional[str] = None,
    chat_link: Optional[str] = None,
    conn: Optional[sqlite3.Connection] = None,
    min_thread_messages: Optional[int] = None,
) -> List[str]:
    if not cfg.enable_ai_summary:
        return []

    lines = ["", "---", "", "## ğŸ¤– æ™ºèƒ½è¯é¢˜æ‘˜è¦"]

    if not cfg.ai_api_base:
        lines.append("âš ï¸ **AI æ‘˜è¦æœªç”Ÿæˆ**ï¼šç¼ºå°‘ `ai_api_base` é…ç½®")
        log.warning("AI summary enabled but ai_api_base not set.")
        return lines

    if not cfg.ai_api_key:
        lines.append("âš ï¸ **AI æ‘˜è¦æœªç”Ÿæˆ**ï¼šç¼ºå°‘ `ai_api_key` é…ç½®")
        log.warning("AI summary enabled but ai_api_key not set.")
        return lines
    
    if not conn:
        lines.append("âš ï¸ **AI æ‘˜è¦æœªç”Ÿæˆ**ï¼šç¼ºå°‘æ•°æ®åº“è¿æ¥")
        log.warning("AI summary enabled but database connection not provided.")
        return lines

    # ç¡®å®šä½¿ç”¨çš„æœ€å°æ¶ˆæ¯æ•°é‡é˜ˆå€¼ï¼šä¼˜å…ˆä½¿ç”¨ç¾¤ç»„ç‰¹å®šé…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é»˜è®¤å€¼
    threshold = min_thread_messages if min_thread_messages is not None else MIN_THREAD_MESSAGES

    # æŒ‰ thread_id åˆ†ç»„æ¶ˆæ¯
    threads = _group_messages_by_thread(rows)

    # è¿‡æ»¤æ‰æ¶ˆæ¯æ•°é‡å°äºé˜ˆå€¼çš„çº¿ç¨‹
    valid_threads = {tid: msgs for tid, msgs in threads.items() if len(msgs) >= threshold}

    if not valid_threads:
        lines.append(f"*æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„çº¿ç¨‹ï¼ˆæ¶ˆæ¯æ•°é‡ >= {threshold}ï¼‰*")
        return lines

    lines.append(f"**å…± {len(valid_threads)} ä¸ªçº¿ç¨‹ç¬¦åˆåˆ†ææ¡ä»¶**")
    lines.append("")

    tz_name = getattr(cfg.timezone, "key", None) or str(cfg.timezone)

    # åˆ›å»ºæ¶ˆæ¯IDåˆ°æ¶ˆæ¯è¯¦æƒ…çš„æ˜ å°„ï¼ˆåŒ…æ‹¬æ–‡æœ¬å†…å®¹ï¼‰
    message_map: Dict[int, Dict[str, Any]] = {}
    for row in rows:
        message_map[row["message_id"]] = {
            "text": row["text"] or "",
            "user_id": row["user_id"],
            "username": row["username"],
        }

    # ä¸ºæ¯ä¸ªç¬¦åˆæ¡ä»¶çš„çº¿ç¨‹åˆ†åˆ«è°ƒç”¨ AI åˆ†æ
    for thread_id, thread_rows in sorted(valid_threads.items(), key=lambda x: len(x[1]), reverse=True):
        thread_name = "é¡¶å±‚æ¶ˆæ¯" if thread_id == TOP_THREAD_ID else f"çº¿ç¨‹ {thread_id}"
        total_messages = len(thread_rows)
        lines.append(f"### ğŸ’­ {thread_name}ï¼ˆ{total_messages} æ¡æ¶ˆæ¯ï¼‰")
        lines.append("")

        # å¦‚æœæ¶ˆæ¯æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œåˆ†æ®µå¤„ç†
        if total_messages > cfg.ai_max_messages_per_batch:
            batch_lines = _process_thread_batch(
                thread_rows, thread_id, cfg, day_start, tz_name, chat_id, chat_name, chat_type, chat_link, message_map, conn
            )
            lines.extend(batch_lines)
        else:
            # æ¶ˆæ¯æ•°é‡ä¸å¤šï¼Œç›´æ¥å¤„ç†
            single_lines = _process_single_thread(
                thread_rows, thread_id, cfg, day_start, tz_name, chat_id, chat_name, chat_type, chat_link, message_map, conn
            )
            lines.extend(single_lines)

        lines.append("")

    return lines
